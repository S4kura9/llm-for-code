# llm-for-code
## Collection on Large Language Models for Code Generation


## LLM (before Codex)
| Name|Date                                         |Link                                     |method |
|----------------------|--------------------------------|-------------------------------------------------|-----|
|Transformer|2017.6|[Attention is all you need](https://arxiv.org/abs/1706.03762)|attention|
|GPT|2018.6|[Improving Language Understanding by Generative Pre-Training](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)|pretraining + fine tuning|
|BERT|2018.10|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) |双向编码|
|GPT-2|2019.2|[Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe) |zero-shot+prompt engineering|
|T5|2019.11|[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)|Text-to-Text|
|GPT-3|2020.5|[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) |few-shot+prompt engineering|

## LLM for coding
| Name|Date                                         |Link                                     |method |
|----------------------|--------------------------------|-------------------------------------------------|-----|
|Codex|2021.7|[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)| 预训练+微调 |
|AlphaCode|2022.3|[Competition-Level Code Generation withAlphaCode](https://www.semanticscholar.org/paper/Competition-level-code-generation-with-AlphaCode-Li-Choi/5cbe278b65a81602a864184bbca37de91448a5f5)| 预训练+微调|
|CodeGeeX|2023.8|[CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X](https://arxiv.org/abs/2303.17568)| 预训练+微调|
|PET|2020.1|[Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/abs/2001.07676)|提示学习|
|CodeT5|2021.9|[CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/abs/2109.00859v1)|Text-to-Text|
|CodeT5+|2023.5|[CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922)|InstructCodeT5基于其指令微调|
|Code Llama|2023.8|[Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)|指令微调|
|RLHF|2019.9|[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)|RL|
|CodeRL|2022.7|[CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://arxiv.org/abs/2207.01780)|RL|
|RAG|2020.5|[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)|RAG|
|MetaGPT|2023.8|[MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework](https://arxiv.org/abs/2308.00352)|多智能体|
|MAD|2023.5|[Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate](https://arxiv.org/abs/2305.19118)|多智能体|

## LLM for MILP
| Name|Date                                         |Link                                     |
|----------------------|--------------------------------|-------------------------------------------------|
|OptiMUS|2023.11|[OptiMUS: Optimization Modeling Using MIP Solvers and large language models](https://arxiv.org/abs/2310.06116)|
|FunSearch|2023.11|[Mathematical discoveries from program search with large language models](https://www.nature.com/articles/s41586-023-06924-6)|
|EOH|2024.1|[Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model](https://arxiv.org/abs/2401.02051)|
|MILP-Evolve|2024.11|[ TOWARDS FOUNDATION MODELS FOR MIXED INTE GER LINEAR PROGRAMMING](https://arxiv.org/abs/2410.08288)|
|OPRO|2023.9|[Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)|
|Ecole|2020.11|[Ecole: A Gym-like Library for Machine Learning in Combinatorial Optimization Solvers](https://arxiv.org/abs/2011.06069)|
|PoH|2025.2|[planning of heuristics: strategic planning on large language models with Monte Carlo Tree Search for Automating Heuristic Optimization ](https://arxiv.org/abs/2502.11422)|